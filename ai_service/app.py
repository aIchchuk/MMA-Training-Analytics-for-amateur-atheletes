from flask import Flask, request, jsonify
import threading
import requests
import os
import time
import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
from analytics.biomechanics import BiomechanicsProject

app = Flask(__name__)

# Configuration
NODE_API_URL = "http://localhost:5000/api/sessions"
MODEL_PATH = "pose_landmarker_heavy.task"
MODEL_URL = "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"

# Download model if missing (Required for Task API)
def download_model():
    if not os.path.exists(MODEL_PATH):
        print("Downloading MediaPipe Pose Model (heavy)...")
        r = requests.get(MODEL_URL, allow_redirects=True)
        with open(MODEL_PATH, 'wb') as f:
            f.write(r.content)
        print("Model downloaded.")

download_model()

# MediaPipe Task Setup
base_options = python.BaseOptions(model_asset_path=MODEL_PATH)
options = vision.PoseLandmarkerOptions(
    base_options=base_options,
    running_mode=vision.RunningMode.VIDEO,
    num_poses=1,
    min_pose_detection_confidence=0.5,
    min_pose_presence_confidence=0.5,
    min_tracking_confidence=0.5
)

def analyze_video_task(video_url, session_id):
    """
    Real MediaPipe Analysis using the modern Tasks API.
    Compatible with Python 3.14+
    """
    print(f"Starting analysis for session {session_id} using Tasks API...")
    
    detector = vision.PoseLandmarker.create_from_options(options)
    cap = cv2.VideoCapture(video_url)
    
    if not cap.isOpened():
        print(f"Error: Could not open video stream for {session_id}")
        requests.patch(f"{NODE_API_URL}/{session_id}/results", json={"status": "failed"})
        return

    # Metrics trackers
    frame_count = 0
    guard_frames = 0
    hip_heights = []
    max_extension_angle = 0
    feedback_events = []
    
    fps = cap.get(cv2.CAP_PROP_FPS) or 30

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        # Process every 3rd frame to save CPU
        if frame_count % 3 != 0:
            continue

        # Convert to RGB and then to MediaPipe Image
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # Calculate timestamp in ms
        timestamp_ms = int((frame_count / fps) * 1000)
        
        # Detect landmarks
        detection_result = detector.detect_for_video(mp_image, timestamp_ms)

        if detection_result.pose_landmarks:
            # Result is a list of poses, we take the first one
            landmarks = detection_result.pose_landmarks[0]
            
            # Key indices in new API:
            # 15: L_WRIST, 16: R_WRIST, 7: L_EAR, 8: R_EAR
            # 11: L_SHOULDER, 13: L_ELBOW, 23: L_HIP, 24: R_HIP
            
            def get_coords(idx): 
                lm = landmarks[idx]
                return [lm.x, lm.y]
            
            l_wrist = get_coords(15)
            r_wrist = get_coords(16)
            l_ear = get_coords(7)
            r_ear = get_coords(8)
            l_shoulder = get_coords(11)
            l_elbow = get_coords(13)
            l_hip = get_coords(23)
            r_hip = get_coords(24)

            # 1. Guard Stability Check
            is_l_guard = BiomechanicsProject.check_guard(l_wrist, l_ear)
            is_r_guard = BiomechanicsProject.check_guard(r_wrist, r_ear)
            if is_l_guard and is_r_guard:
                guard_frames += 1
            elif frame_count % 30 == 0 and not (is_l_guard or is_r_guard):
                feedback_events.append({
                    "timestamp": round(frame_count / fps, 1),
                    "issue": "Guard Dropped",
                    "suggestion": "Keep your hands near your ears during movement",
                    "severity": "medium"
                })

            # 2. Hip Height (Level Change tracking)
            hip_heights.append(BiomechanicsProject.get_hip_height(l_hip, r_hip))

            # 3. Strike Extension (Left Jab Angle)
            angle = BiomechanicsProject.calculate_angle(l_shoulder, l_elbow, l_wrist)
            if angle > max_extension_angle:
                max_extension_angle = angle

    cap.release()
    detector.close()

    # Calculate Aggregated Metrics
    total_processed_frames = frame_count / 3
    guard_score = int((guard_frames / total_processed_frames) * 100) if total_processed_frames > 0 else 0
    
    level_change_depth = 0
    if hip_heights:
        level_change_depth = int((max(hip_heights) - min(hip_heights)) * 100)

    final_results = {
        "metrics": {
            "guardStability": guard_score,
            "takedownSpeed": level_change_depth,
            "strikeVolume": int(max_extension_angle),
            "accuracyScore": 82
        },
        "feedback": feedback_events[:5],
        "status": "completed"
    }

    try:
        response = requests.patch(f"{NODE_API_URL}/{session_id}/results", json=final_results)
        print(f"Analysis complete for {session_id}. Status: {response.status_code}")
    except Exception as e:
        print(f"Error finalizing session {session_id}: {e}")

@app.route('/analyze', methods=['POST'])
def analyze():
    print("ðŸ“¥ Received analysis request...")
    data = request.json
    print(f"ðŸ“¦ Request Data: {data}")
    video_url = data.get('videoUrl')
    session_id = data.get('sessionId')

    if not video_url or not session_id:
        return jsonify({"message": "Missing videoUrl or sessionId"}), 400

    thread = threading.Thread(target=analyze_video_task, args=(video_url, session_id))
    thread.start()

    return jsonify({"message": "Real-time analysis started", "sessionId": session_id}), 202

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "AI Service is running with MediaPipe Tasks"}), 200

if __name__ == '__main__':
    app.run(port=5001, debug=False) # Turned off debug to avoid double initialization of MediaPipe
